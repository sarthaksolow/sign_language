This project implements real-time Sign Language Recognition (SLR) using deep learning techniques to translate hand gestures into text or speech. It is designed to assist individuals with hearing and speech impairments by enabling seamless communication through AI-driven gesture recognition.

Features
 Real-time Hand Gesture Detection: Uses MediaPipe Hands or OpenCV to track hand landmarks.
 Sign Classification: Trained a deep learning model (CNN, LSTM, or Transformers) to recognize different sign language gestures.
 Live Video Processing: Detects and classifies signs from webcam input.

 Custom Dataset Support: Can be trained on different sign languages (ASL, ISL, BSL, etc.)
